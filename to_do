	                         -*- text -*-

Bugs and imediate improvements:

o  save and restore models.

o  Quit using the first 10k over and over. Go through the rest
   of the data instead.  This makes it very close to SGD.

Future logic:

o  Add the ability to do continous contrasts.  Namely, don't break into
   sentence.  Just use free flowing text, html tags and everything.  Then
   every 10 or so words, try a different direction and let it rip for 20
   words.  Now compute the likelihood of both paths.  Be careful of
   pushing stuff to the future--so links that never come down should be
   penalized. 

o  Added the ability to make a new eigen-dictionary based off of
   bigrams computed between parent-child pairs.

Experiments:

o  Find some labelled data to run on.  Write a likelihood estimator based
   on the correct parse.  Number of edges correct?  Right direction of
   dependence? guess as to distance?

o  Make limited strategies: all left.  all right.  stack limited.  Use
   these as strawmen.

Runs:

See what the coverage is for our dictionary.

Build a new google grams dictionary which doesn't downcase.  This
might help with understanding start of sentences etc.

Parse wiki into sentences and run on that.

LOW PRIORITY

speedup / storage: Divide up eigen dictionary into two pieces, the
tokenization part (std::string) and the index part (int).  Then only
pass around the tokenized version.  So an Eigen_Dictionary now takes a
"word" which is represented as an integer.  Basically this can all be
done by modifying the basic "Word" class.

speedup: There is a map in value_of_forecasts which should be replaced
by an vector for speed.

speedup: profile code to see what actually is taking time.

speedup: compute matrix*vector for parser to generate new vector.
This should make parsing instatnious.  Since this isn't actually the
slow step, it may be more for show.
