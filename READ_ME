				R E A D    M E		 -*- text -*-

 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

Conventions:

Arcs:	Arc's are described as Left_location, direction_of_arc,
	right_location.  (This will be checked by left_location <
	right_location.)

Arcs:	Link(a,b) = a --> b, or b <-- a.  So a is parent, b is dependent.

Arrows: A simple tree with three nodes might have:

		Root --> A
		A    --> B
		A    --> C

	The point is that flow is from the root to the children.
	Sometimes this is drawn the other way around.

Actions:
	shift = push
	shift_eager = push (different legality condition)
	left_eager = top depends on queue.  Pop
	right_eager = queue depends on top.  push
	left_reduce = stack(1) depends on stack(0). Pop
	right_reduce = stack(0) depends on stack(1). Pop
	head_reduce = pop (but only allowed as last possible action)
		(called simply "reduce" in eager version)
	right_2 = stack(0) depends on stack(2). pop
	right_3 = stack(0) depends on stack(3). pop

	Standard actions:
		shift, left_reduce, right_reduce, head_reduce.
	Arc-Eager actions:
		shift_eager, left_eager, right_eager, head_reduce.
	To generate non-projective, include right_2, right 3, etc.

Searn in practice (Daume, Langford, Marcu)
	left_reduce
	right_reduce
	shift
	(Ends with a single item on the stack)



 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

Overall work flow.

(1) External to this code, tokenize the corpus.

(2) External to this code, create an eigen dictionary. 

(3) Loop the following (done by learn.main.cc)

	(a) Parse sentences using LR model
	(b) generate contrasts of alternatives parses
	(c) compute the likelihood for each of these parse
	(d) put into a database of alternatives
	(e) learn a better LR model from this database
	(f) Reparse all sentence using improved LR model
	(g) Use these parsed sentences to fit a new likelihood

 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

				Classes		

Each class is in a file of that name.  (A few are in the wrong file,
so the file where they are found is in parenthesis.)  There are some
things that are called from the utlities library (my_random, rounding,
pretty boxing of output), these aren't described below.

		Preprocessing

Tokenize		Does simple conversion of string to "Words".
			Much of the actual tokenization was done with
			a preprosess step which included emacs.    :-(
Eigenwords		Reads in "pretty_*.csv" file

		Basic

Word			Basic word
Words (word)		A sentence
Action (history)	Basic possible parse actions
Value_of_forecasts	Each action is given a value

		Parsing

Dependency		The parse itself
   Decorated_dependency (version with probabilities)
LR			A parser (without an Oracle)
History			A sequence of Actions 
Redo_parse		A controller that will play a History on an LR

		Oracle

Statistical_parse	An Oracle 
Model			Model used by Oracle
Feature			ABC of features
   Feature_eigenwords
   Feature_interaction
   feature_one_dimensional (sentence_length,number_words_left,stack_size)
   Feature_shorten
Feature_generator	Stores a collection of features
Forecast		Predicts action to take (pieces of the Model)
   Forecast_constant	Example forecast
   Forecast_linear	linear weights of features
Value_of_forecasts	Bundle of the value of several actions

		Evaluation

Likelihood		likelihood of a dependency parse
Transition_probability	Transition probabilities used in likelihood
    TP_eigenwords	Uses eigenwords to compute Transition probabilities

		Learning

Learn			Main learning program
Maximum_likelihood	processes dependency parses and fits the likelihood
Train_forecast_linear	Does the X'X accumulation I think.  Why a whole class to itself?
Contrast		Builds a database using LR and a Model
   Row			One "row" generated by contrast (actually a pair of rows)
   suggest_alternative_history (now only a member function of Contrast)


